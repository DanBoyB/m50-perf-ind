---
title: "Processing M50 C2 data"
author: "Dan Brennan"
date: "14 August 2017"
output: github_document
---

This note outlines the process and code used to read the M50 PVR data and output aggregated tables and calculation of performance indicators.

```{r setup, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(knitr)
library(aws.s3)

```

### Global Variables

Initially global variables are defined including the path to the location where the PVR records are stored, the year required, the route number (in this case 50 for M50) and the TMU site numbers that are required.

A connection is then opened to the SQLite database to store both the raw PVR records and the aggregated tables (change create = FALSE once database is created).

```{r}
data_dir <- "~/datastore/"
analysis_dir <- "~/R/projects/m50-perf-ind/"
year <- 2015
route <- 50
site_no <- c(1501, 1502, 1503, 1504, 1505, 1506, 1507, 
              1508, 1509, 15010, 15011, 15012)

names <- c("siteID", "RecTime", "SubSec", "Flownum", "Direction", 
             "Speed", "Class", "Headway", "Gap", "Length", "GrossWeight", 
             "NumAxles", "ESAL", "Axles", "Weather", "VehicleId", 
             "ClassScheme", "Flag", "BSEQ", "Code", "LegalStatus", 
             "Validity", "ANPRConfidence", "ImagesPresent", "siteID2", "routeNo")
```

### Define Functions

1. Create calendar with unix epoch values for start and end of month

This is used to split data into monthly subsets in database and filter dates in SQLite before loading into memory for aggregation using R.

```{r}
create_unix_cal <- function(y) {
  unix_cal <- tibble(year = rep(y, 12), month = formatC(1:12, width = 2, flag = "0"), 
                    days = 0, start = 0, end = 0) %>%
  mutate(days = as.character(days_in_month(as.POSIXct(paste(year, month, "01", sep = "-"), 
                                                      format = "%Y-%m-%d"))),
         start = as.POSIXct(paste(year, "-", month, "-", "01", " 00:00:00", sep = "")),
         end = as.POSIXct(paste(year, "-", month, "-", days, " 23:59:59", sep = "")),
         start_unix = unclass(start),
         end_unix = unclass(end))

unix_cal <- unix_cal %>% 
  select(start_unix, end_unix, month)

}

unix_df <- create_unix_cal(year)

```

2. Read PVR flat files and output monthly csv files

The PVR files for the M50 in 2014 are stored in the Amazon S3 "ie-tii-data-internal/pvrs/".

The PVR records are split into 4 flat text files per site. Due to data size, if all sites were processed together, the subsequent aggregation functions would require more memory.

A function is created to point to the s3 object, read in the flat files into memory, loop through each month and output to monthly csv files. If a full month of data is missing at a site, the process will skip this and return a message.

```{r}
pvr_to_monthly <- function(year, route, site_no) {
    
    files <- c("0000_part_00", "0001_part_00", "0002_part_00", "0003_part_00")
    
    lapply(files, function(i) by(unix_df, 1:nrow(unix_df), function(row) {
        a <- s3read_using(FUN = read_tsv, 
                          col_names = names, 
                          object = paste0("s3://ie-tii-data-internal/pvrs/", 
                                          year, "/route-", 
                                          route, "/site-",                                          
                                          site_no, 
                                          "/", 
                                          i)) %>% 
            filter(RecTime >= row[[1]] & RecTime <= row[[2]])
        
        if (nrow(a) == 0) {
            message(paste("No data available at site", 
                          site_no,
                          "for",
                          month(as.POSIXct(row[[1]], origin = "1970-01-01"),
                                label = TRUE, abbr = FALSE), 
                          year, 
                          "\n", 
                          sep = " "))
            Sys.sleep(2)
            return (NULL)
            } 
        
        else {
            a %>% 
                write_csv(paste(analysis_dir, 
                                paste0("output/pvr/", "pvr-", route, "-", 
                                       site_no, "-", row[3], "-", year), 
                                ".csv", 
                                sep = ""),
                    append = TRUE)
        }
          
        rm(a)
        gc(verbose = FALSE) }))
    
}
```

3. Data Aggregation

A function to aggregate data into the required periods is developed. This loops through the csv files for each site and each month, reads into memory, groups into the period selected, defines new columns and outputs to a new aggregated csv file. 


```{r}
aggregate_pvr <- function(site_no, route, year, 
                         aggregation = c("day", "hour", "min15", "min5"), 
                         month) {
    
  
  secs <- ifelse(aggregation == "day", 86400,
       ifelse(aggregation == "hour", 3600,
       ifelse(aggregation == "min15", 900,
       ifelse(aggregation == "min5", 300, NA))))
  
  file_name <- paste(analysis_dir, paste("output/pvr", 
                                    route, site_no, month, year, sep = "_"), 
                    ".csv", sep = "")
  
  if (!file.exists(file_name)) {
      return (NULL)
      }
  
  else {
      s <- read_csv(file_name, col_names = names) %>% 
          arrange(RecTime) %>% 
          select(siteID2, routeNo, RecTime, Code, Direction, 
                 Speed, Class, Length) %>%     
          rename(siteID = siteID2) %>% 
          mutate(time = trunc(as.numeric(RecTime) / secs) * secs,
                 Speed = (Speed / 1000) * 3.6) %>%
          group_by(siteID, time, Class, Code) %>%
          summarise(volume = n(), speed = mean(Speed)) %>% 
          ungroup() %>% 
          write_csv(paste(analysis_dir, paste("output/aggr/", 
                                              aggregation, route, year, sep = "_"),
                          ".csv", sep = ""),
                    append = TRUE)
      }
  
  rm(s)
  gc(verbose = FALSE)
  
  }

```

### Load PVR files and output as monthly csv files

The pvr_to_monthly function is used to loop through all M50 sites and outpuy the PVR records as seperate csv files for each site and each month of 2014. This process takes approx 30 mins when used for all sites on the M50 over a year.


```{r, eval = FALSE}

site_no %>%
    map(pvr_to_monthly, year = year, route = 50)

```

A summary of the structure of a database PVR table is shown below.

```{r}
glimpse(read_csv(paste(analysis_dir, "output/pvr_40_1258_01_2015.csv", sep = ""),
                 col_names = names))
```


### Output aggregated tables

The aggregate_pvr function is used to produce the required aggregated tables for daily, hourly, 15 minute and 5 minute intervals. 

```{r, eval = FALSE}
lapply(siteNo, function(x) {
#daily
lapply(unix_df$month, aggregate_pvr, 
       site_no = x, route = route, year = year, aggregation = "day")
#hourly
lapply(unix_df$month, aggregate_pvr, 
       site_no = x, route = route, year = year, aggregation = "hour")
#15 min
lapply(unix_df$month, aggregate_pvr, 
       site_no = x, route = route, year = year, aggregation = "min15")
#5 min
lapply(unix_df$month, aggregate_pvr, 
       site_no = x, route = route, year = year, aggregation = "min5")
})
```

Column names are added to the csv files.

```{r}
aggregation <- c("day", "hour", "min15", "min5")

aggregation %>% 
    map(function(x) {

    file_name <- paste(analysis_dir, "output/", x, "_40_2015.csv", sep = "")
                    
    if (!file.exists(file_name)) {
        return (NULL)
    }
    
    else {
        read_csv(file_name, col_names = c("siteID", "time", "Class", "Code", "volume", "speed")) %>% 
            write_csv(file_name)    
    }
    
    })
```


As a check, the output daily aggregated traffic data was examined for site 1012

```{r}
hourly <- read_csv(paste(analyDir, "output/hourly_50_2014.csv", sep = "")) %>% 
  filter(siteID == 1012) %>% 
  collect(n = Inf) %>% 
  mutate(time = as.POSIXct(time, origin = "1970-01-01")) %>% 
  arrange(time) %>% 
  group_by(siteID, time, Class) %>% 
  summarise(volume = sum(volume), AvgSpeed = mean(speed)) %>% 
  filter(Class != 0)

hourly %>%
  head(10) %>% 
  kable()
```

This is consistent with data on the  [C2 wesbite](https://www.nratrafficdata.ie/c2/tfreport.asp?node=NRA&cosit=000000001012&reportdate=2015-01-01&enddate=2015-01-01&dimtype=2) 

***

![](c2Hour.jpeg)

***

Similarily daily, 5 and 15 minute aggregated data is consistent with data on C2.

## Supplementary Data

Create a reference table for vehicle classes

```{r, eval = FALSE}
classes <- data_frame(Class = 1:7) %>% 
  mutate(mode = ifelse(Class == 1, "mbike",
                ifelse(Class == 2, "car",
                ifelse(Class == 3, "lgv",
                ifelse(Class == 4, "bus",
                ifelse(Class == 5, "hgvRigid",
                ifelse(Class == 6, "hgvArtic",
                ifelse(Class == 7, "caravan", NA))))))))

classes %>% 
   write_csv(paste(analysis_dir, "output/classes.csv", sep = ""))
```


